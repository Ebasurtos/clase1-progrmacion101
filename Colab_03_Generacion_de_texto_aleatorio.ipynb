{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ebasurtos/clase1-progrmacion101/blob/main/Colab_03_Generacion_de_texto_aleatorio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Generación de texto aleatorio\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=https://drive.google.com/uc?export=view&id=1NpWawsHQE7YaTtWFlwO4Y0oDkHyb6HFE width=\"500\">\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "\n",
        "### Librería *nltk*\n",
        "\n",
        "https://www.nltk.org/book/"
      ],
      "metadata": {
        "id": "UD8Eqi1ZhbRD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SXidBOpKP7n",
        "outputId": "68f4fd82-e970-4934-de51-a53568dc50ee",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('book')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos algunos textos"
      ],
      "metadata": {
        "id": "FZzft9Cjhgg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "JVMVrU5nKTna",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae126f3-9031-464f-8cb1-a0f3c2989152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNcHyBCdLgVl",
        "outputId": "5aa4f5bb-577f-4398-b1be-170aaae5f14f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploramos el texto correspondiente a la novela *Sense and Sensibility*"
      ],
      "metadata": {
        "id": "lmpd5jDphifm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUZhcmdyotaz",
        "outputId": "7e2a9716-eec9-469c-9123-433e7dbda3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Text: Sense and Sensibility by Jane Austen 1811>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(text2)[0:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSHddA6aQ6Pk",
        "outputId": "f5648848-ac0d-4cce-ae98-fa5775cbe61f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Sense',\n",
              " 'and',\n",
              " 'Sensibility',\n",
              " 'by',\n",
              " 'Jane',\n",
              " 'Austen',\n",
              " '1811',\n",
              " ']',\n",
              " 'CHAPTER',\n",
              " '1',\n",
              " 'The',\n",
              " 'family',\n",
              " 'of',\n",
              " 'Dashwood',\n",
              " 'had',\n",
              " 'long',\n",
              " 'been',\n",
              " 'settled',\n",
              " 'in']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGpS8pbWUMPe",
        "outputId": "164e9c60-9c35-431c-dd2b-8dce0f7c1a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "141576"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos la lista de tokens"
      ],
      "metadata": {
        "id": "zkLyyLVJhphU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=text2.tokens\n",
        "words[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4yGi02aT8QJ",
        "outputId": "6ab17e04-3ab4-4f74-f58c-73ca60a64f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Sense',\n",
              " 'and',\n",
              " 'Sensibility',\n",
              " 'by',\n",
              " 'Jane',\n",
              " 'Austen',\n",
              " '1811',\n",
              " ']',\n",
              " 'CHAPTER']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos crear tokens de cualquier texto"
      ],
      "metadata": {
        "id": "PeouGwD-2vnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "oTYrWuI6qktk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(\"Hola buen dia\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdxFpaAbqldL",
        "outputId": "16dcb5f2-6cb6-4436-f5aa-751cb149b485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola', 'buen', 'dia']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos la frecuencia de cada *token*. *vocab* nos brinda un diccionario de cada *token* y la frecuencia con la cual ocurre."
      ],
      "metadata": {
        "id": "0ASyAfDKhtY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2.vocab()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrydkTKgXP2b",
        "outputId": "4db732d1-9acb-4436-fead-c1ce9007cd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({',': 9397, 'to': 4063, '.': 3975, 'the': 3861, 'of': 3565, 'and': 3350, 'her': 2436, 'a': 2043, 'I': 2004, 'in': 1904, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2.vocab()[\"the\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENwoThvPXV91",
        "outputId": "1a044a21-5b31-454b-f013-c3b9cf56b9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3861"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cantidad de *tokens* distintos."
      ],
      "metadata": {
        "id": "VGkEgfMJhyHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(set([word.lower() for word in words]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcsY5OosWEKW",
        "outputId": "0fb91b5a-7424-426f-949b-5940430c3e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6403"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cantidad de *tokens* distintos que únicamente tienen letras (omitimos signos de puntuación)"
      ],
      "metadata": {
        "id": "U5TyK6933Vp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(set([word.lower() for word in words if word.isalpha()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6HBZImOWQ9_",
        "outputId": "1a7f044c-a736-48b1-c18a-979c0c2d8624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6283"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lista de palabras (omitimos signos de puntuación) en minúsculas."
      ],
      "metadata": {
        "id": "Xcj7kpg63fSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_texto=[word.lower() for word in words if word.isalpha()]\n",
        "len(words_texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKW11s3LWfYw",
        "outputId": "6c955cb9-fab6-459a-8d41-d5fc798a931d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120733"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_texto[0:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5ndzt5k3wgQ",
        "outputId": "8473c21a-6d16-4193-e536-34d47077a45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sense',\n",
              " 'and',\n",
              " 'sensibility',\n",
              " 'by',\n",
              " 'jane',\n",
              " 'austen',\n",
              " 'chapter',\n",
              " 'the',\n",
              " 'family',\n",
              " 'of',\n",
              " 'dashwood',\n",
              " 'had',\n",
              " 'long',\n",
              " 'been',\n",
              " 'settled',\n",
              " 'in',\n",
              " 'sussex',\n",
              " 'their',\n",
              " 'estate',\n",
              " 'was']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frecuencias de estas palabras"
      ],
      "metadata": {
        "id": "zdV5mYA93pUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FreqDist(words_texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9Egv6i-XB_u",
        "outputId": "363231ee-1b4c-497e-c24f-f0fc621f293a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'to': 4116, 'the': 4105, 'of': 3572, 'and': 3491, 'her': 2551, 'a': 2092, 'i': 2004, 'in': 1979, 'was': 1861, 'it': 1757, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigramas\n",
        "\n",
        "Un bigrama es un conjunto de dos palabras consecutivas. La función *bigrams* crea una lista de todos los bigramas que aparecen en un texto."
      ],
      "metadata": {
        "id": "X4HXwFSJiHK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(bigrams([\"hola\", \"que\", \"tal\"])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uly8EF4CKY9j",
        "outputId": "5b792636-586c-493e-a1a4-5ca638d8a47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hola', 'que'), ('que', 'tal')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigramas=bigrams(words_texto)"
      ],
      "metadata": {
        "id": "OV5yvYTkMdFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diccionario de frecuencias de bigramas"
      ],
      "metadata": {
        "id": "1KkQ72eBiMfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq=nltk.FreqDist(bigramas)"
      ],
      "metadata": {
        "id": "K8V8mjj5Xm2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfVHYytVZH0f",
        "outputId": "77d08380-851b-459f-f699-d2d896222d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({('to', 'be'): 436, ('of', 'the'): 430, ('in', 'the'): 359, ('it', 'was'): 280, ('of', 'her'): 276, ('to', 'the'): 242, ('to', 'her'): 230, ('mrs', 'jennings'): 230, ('i', 'am'): 224, ('she', 'was'): 209, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imprimimos aquellos bigramas que empiezan con \"mrs\" y sus frecuencias"
      ],
      "metadata": {
        "id": "Y1xn-UC94Dt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palabra2=[]\n",
        "pesos=[]\n",
        "for key,value in freq.items():\n",
        "  if key[0]==\"mrs\":\n",
        "    palabra2.append(key[1])\n",
        "    pesos.append(value)\n",
        "    print(key, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKGnq_uwZL4p",
        "outputId": "c89628de-f7f1-4d67-bcc1-f3e64845cde0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('mrs', 'henry') 1\n",
            "('mrs', 'john') 25\n",
            "('mrs', 'dashwood') 121\n",
            "('mrs', 'ferrars') 75\n",
            "('mrs', 'jennings') 230\n",
            "('mrs', 'smith') 17\n",
            "('mrs', 'palmer') 38\n",
            "('mrs', 'jenning') 4\n",
            "('mrs', 'taylor') 2\n",
            "('mrs', 'ellison') 3\n",
            "('mrs', 'brandon') 2\n",
            "('mrs', 'willoughby') 3\n",
            "('mrs', 'dennison') 1\n",
            "('mrs', 'clarke') 1\n",
            "('mrs', 'richardson') 2\n",
            "('mrs', 'edward') 1\n",
            "('mrs', 'mrs') 1\n",
            "('mrs', 'robert') 2\n",
            "('mrs', 'burgess') 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "palabra2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j9TtG0fHeXQ",
        "outputId": "ecd127b8-21db-47cb-dae1-a06b7b9b628b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['henry',\n",
              " 'john',\n",
              " 'dashwood',\n",
              " 'ferrars',\n",
              " 'jennings',\n",
              " 'smith',\n",
              " 'palmer',\n",
              " 'jenning',\n",
              " 'taylor',\n",
              " 'ellison',\n",
              " 'brandon',\n",
              " 'willoughby',\n",
              " 'dennison',\n",
              " 'clarke',\n",
              " 'richardson',\n",
              " 'edward',\n",
              " 'mrs',\n",
              " 'robert',\n",
              " 'burgess']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.choices(palabra2, pesos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtiVsLVkhZ8H",
        "outputId": "e855e307-ab78-4315-9394-f5626fe7c7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jennings']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum=0\n",
        "for key,value in freq.items():\n",
        "  if key[0]==\"mrs\":\n",
        "    sum+=value\n",
        "sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geUbHeD56rrD",
        "outputId": "21dff329-df95-4464-c886-d52a22ce458f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "530"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distribución empírica"
      ],
      "metadata": {
        "id": "HjPfULykGo-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.array(pesos)/sum"
      ],
      "metadata": {
        "id": "Ag38WCafVar_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037ccead-4484-44f9-afb2-7cd5ab68bd7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00188679, 0.04716981, 0.22830189, 0.14150943, 0.43396226,\n",
              "       0.03207547, 0.07169811, 0.00754717, 0.00377358, 0.00566038,\n",
              "       0.00377358, 0.00566038, 0.00188679, 0.00188679, 0.00377358,\n",
              "       0.00188679, 0.00188679, 0.00377358, 0.00188679])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo de cadena de Markov\n",
        "\n",
        "En un modelo de cadena de Markov, la probabilidad de un texto\n",
        "\n",
        "$$w_1 w_2 ... w_n$$\n",
        "\n",
        "donde $w_i$ es una palabra, se calcula como el producto de la probabilidad de que aparezca la palabra $w_1$ por la probabilidad de transición de la palabra $w_i$ a la palabra $w_{i+1}$ (para $i=1, ..., n-1$).\n",
        "\n",
        "## **Ejercicio 1:**\n",
        "Asumiendo un modelo de cadena de Markov: ¿cuál es la probabilidad de obtener la frase\n",
        "\"mrs dashwood gave me employment\""
      ],
      "metadata": {
        "id": "-Mo_S_UEHJAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generación de texto\n",
        "\n",
        "Texto aleatorio utilizando el modelo de cadena de Markov del lenguage."
      ],
      "metadata": {
        "id": "WbfuZteW4Ly5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generador(palabra_inicial, longitud):\n",
        "  palabra=palabra_inicial\n",
        "  texto_generado=[palabra]\n",
        "  for i in range(longitud):\n",
        "    palabra2=[]\n",
        "    pesos=[]\n",
        "    for key,value in freq.items():\n",
        "      if key[0]==palabra:\n",
        "        palabra2.append(key[1])\n",
        "        pesos.append(value)\n",
        "    palabra=random.choices(palabra2, pesos)[0]\n",
        "    texto_generado.append(palabra)\n",
        "  return texto_generado"
      ],
      "metadata": {
        "id": "xU7DFbXX8kDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generador(\"the\", 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Lk1pwis9plD",
        "outputId": "3c008e1a-c012-4a81-99d8-b19336cf3070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'good',\n",
              " 'humour',\n",
              " 'could',\n",
              " 'not',\n",
              " 'very',\n",
              " 'unwilling',\n",
              " 'to',\n",
              " 'any',\n",
              " 'one',\n",
              " 'put']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3-grams\n",
        "\n",
        "Este modelo ya no es una cadena de Markov."
      ],
      "metadata": {
        "id": "64mCnQi9khB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2.generate(length=30, random_seed=21)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "nCZ4M_xi1L3g",
        "outputId": "8138fb7a-94ff-4696-b476-766b510334b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building ngram index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "were assured of the different horrors of the two Miss Steeles , lately\n",
            "arrived at their house , was of course , very much to pretend to know\n",
            "the tenderness\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'were assured of the different horrors of the two Miss Steeles , lately\\narrived at their house , was of course , very much to pretend to know\\nthe tenderness'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio en clase\n",
        "n=10\n",
        "p = 0.2\n",
        "valor1 = 0\n",
        "for x in range(1,6):\n",
        "  b_value = binom.pmf(x, n, p)\n",
        "  total = 10*b_value\n",
        "  print(f\"P(X>={x}) = {total}\")\n",
        "  valor1+=total\n",
        "print(valor1)\n",
        "\n",
        "n=10\n",
        "p = 0.2\n",
        "valor2 = 0\n",
        "for x in range(6,11):\n",
        "  b_value = binom.pmf(x, n, p)\n",
        "  total = 2*x*b_value\n",
        "  print(f\"P(X>={x}) = {total}\")\n",
        "  valor2+=total\n",
        "print(valor2)\n",
        "\n",
        "valor2+valor1"
      ],
      "metadata": {
        "id": "tc0Us-_Z-2qm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}